### 问题整理

#### 航迹断链问题 20240312

无论是单机运行融合程序还是三个服务器交替运行融合程序，都会出现航迹断链问题，融合方法需要改进

#### 航迹偏离问题 20240312

融合后的航迹与单源航迹偏离

* 融合航迹11 关联列表 7869906 920 4014
* 融合航迹7  关联列表 7866367 938 658

#### 融合航迹不显示问题 20240312

20240312 记录 显示程序上对于fhs的单源航迹，其融合后的航迹不显示

20240313 更新 单融合机的时候正常显示融合航迹，切换机器之后丢失目标

* 融合航迹38 关联列表 -1 -1 2781 在3'28秒出现

经排查融合航迹正常更新, 日志也有对应的记录，但是没有显示融合航迹

#### 分中心掉线后局部的融合在重新连接后，与主干网络存在的冲突问题 20240513

模拟了掉线后的分中心接收到新一路雷达的数据

分中心在掉线后, 自身成为工作机, 对于接收到的雷达数据进行融合处理

重新连接后可能会出现离线新建的航迹号与集群航迹号冲突的问题

#### 新增 raft 后使用 protobuf 报错问题 20240710

1. 多重定义问题

```
CMakeFiles/msdp_app.dir/msdp/net/Raft.cpp.o: in function buttonrpc::send(zmq::message_t&)':
/root/MSDP_V0.4/buttonrpc/buttonrpc.hpp:246: multiple definition of buttonrpc::send(zmq::message_t&)'; CMakeFiles/msdp_app.dir/msdp/main.cpp.o:/root/MSDP_V0.4/buttonrpc/buttonrpc.hpp:246: first defined here
```

2. protobuf 中的相关类提示 undefined 

```
/opt/rh/devtoolset-9/root/usr/libexec/gcc/x86_64-redhat-linux/9/ld: CMakeFiles/msdp_app.dir/msdp/net/Raft.cpp.o: in function `Raft::logSerialize(std::vector<LogEntry, std::allocator<LogEntry> > const&, std::string&)':
/root/MSDP_V0.4/msdp/net/Raft.cpp:299: undefined reference to `LogEntryMessage::~LogEntryMessage()'
```

#### 定时器任务问题 20240715

1. 重置定时器后未能执行任务

需要重置定时器的结点：

```
1. candidate: election timeout / change to leader / receive leader heartbeat
2. follower: reply to candidate/leader
3. leader: send heartbeat/change to follower
```

定时器相关代码

```
// 定时器设定在200-400ms随机
std::srand(static_cast<unsigned int>(std::time(nullptr)));
m_timer_event = std::make_shared<TimerEvent>(std::rand() % 201 + 200, false, std::bind(&Raft::handleTimeout, this));
m_main_event_loop->addTimerEvent(m_timer_event);

void Raft::resetTimeout(EventLoop* event_loop, TimerEvent::s_ptr timer_event) {
    event_loop->deleteTimerEvent(timer_event);
    std::srand(static_cast<unsigned int>(std::time(nullptr)));
    timer_event->resetInterval(std::rand() % 201 + 200);
    event_loop->addTimerEvent(timer_event);
}
```

### 问题解决

#### 航迹偏离问题解决 20240313

* Server.cpp发送航迹函数编写问题，一开始是for循环但是丢点, 由于每次pop后size大小都会改变，所以不应该使用for循环，改成了while循环
```
修改前：

// 单源发送到显示程序
for(int i = 0; i < m_fusion.unitTrack_data.size(); ++i) {
    RadarTrack rtData = m_fusion.unitTrack_data.front();
    m_fusion.unitTrack_data.pop_front();

    memset(sdBuf, 0, 2000);
    int flystate = 1;
    double Height = (double)(rtData.Hei * 0.3048);
    memcpy(sdBuf, &(rtData.fX), 8);
    memcpy(sdBuf + 8, &(rtData.fY), 8);
    memcpy(sdBuf + 16, &Height, 8);
    memcpy(sdBuf + 24, &(rtData.vec), 8);
    memcpy(sdBuf + 32, &(rtData.TrackNo), 4);
    memcpy(sdBuf + 36, &(rtData.cource), 8);
    memcpy(sdBuf + 44, &flystate, 4);
    
    m_send_socks[rtData.id - 1]->SendData(sdBuf, 2000);
}

// 融合结果发送到显示程序
for(int i = 0; i < m_fusion.fusionRetList.size(); ++i) {
    RadarTrack repFusion = m_fusion.fusionRetList.front();
    m_fusion.fusionRetList.pop_front();
    memset(sdBuf, 0, 2000);
    int flystate = 1;
    double Height = (double)(repFusion.Hei * 0.3048);
    memcpy(sdBuf, &(repFusion.fX), 8);
    memcpy(sdBuf + 8, &(repFusion.fY), 8);
    memcpy(sdBuf + 16, &Height, 8);
    memcpy(sdBuf + 24, &(repFusion.vec), 8);
    memcpy(sdBuf + 32, &(repFusion.TrackNo), 4);
    memcpy(sdBuf + 36, &(repFusion.cource), 8);
    memcpy(sdBuf + 44, &flystate, 4);

    m_send_socks[3]->SendData(sdBuf, 2000);
}

修改后：

// 单源发送到显示程序
while(!m_fusion.unitTrack_data.empty()) {
    RadarTrack rtData = m_fusion.unitTrack_data.front();
    m_fusion.unitTrack_data.pop_front();

    memset(sdBuf, 0, 2000);
    int flystate = 1;
    double Height = (double)(rtData.Hei * 0.3048);
    memcpy(sdBuf, &(rtData.fX), 8);
    memcpy(sdBuf + 8, &(rtData.fY), 8);
    memcpy(sdBuf + 16, &Height, 8);
    memcpy(sdBuf + 24, &(rtData.vec), 8);
    memcpy(sdBuf + 32, &(rtData.TrackNo), 4);
    memcpy(sdBuf + 36, &(rtData.cource), 8);
    memcpy(sdBuf + 44, &flystate, 4);
    
    m_send_socks[rtData.id - 1]->SendData(sdBuf, 2000);
}

// 融合结果发送到显示程序
while(!m_fusion.fusionRetList.empty()) {
    RadarTrack repFusion = m_fusion.fusionRetList.front();
    m_fusion.fusionRetList.pop_front();
    memset(sdBuf, 0, 2000);
    int flystate = 1;
    double Height = (double)(repFusion.Hei * 0.3048);
    memcpy(sdBuf, &(repFusion.fX), 8);
    memcpy(sdBuf + 8, &(repFusion.fY), 8);
    memcpy(sdBuf + 16, &Height, 8);
    memcpy(sdBuf + 24, &(repFusion.vec), 8);
    memcpy(sdBuf + 32, &(repFusion.TrackNo), 4);
    memcpy(sdBuf + 36, &(repFusion.cource), 8);
    memcpy(sdBuf + 44, &flystate, 4);

    m_send_socks[3]->SendData(sdBuf, 2000);
}
```

* m_sys_time编写错误, 修改

```
修改前：
if(m_fusion.m_sys_time = 0) {
    m_fusion.m_sys_time = getNowMs();
} else {
    m_fusion.m_sys_time += Config::get_instance()->m_fusion_period;
}

修改后：
if(m_fusion.m_sys_time == 0) {
    m_fusion.m_sys_time = getNowMs();
} else {
    m_fusion.m_sys_time += Config::get_instance()->m_fusion_period;
}
```

* 权重计算修改
```
原本为了避免dis == 0的情况，修改了计算公式，但是导致了航迹的偏离，不明原因，因此改了另一种方法

修改前：
fu.assMap[radarNo].weight = (fu.assMap[radarNo].weight * (size - 1) + 1 / (dis + 1)) / size;

修改后：
if(dis > 1e-6) {
    // 上面的外推过程可能存在问题，会有重复的报点导致dis为0, 需要跳过这种情况
    fu.assMap[radarNo].weight = (fu.assMap[radarNo].weight * (size - 1) + 1 / dis) / size;
}
```

#### 分中心掉线重连后主干网络存在的冲突问题 20240513

##### 目前的处理逻辑: 

1. 当服务器A掉线时, 记录服务器A脱离主干网络时接收的的最后一个主干网络的新建航迹号, 记为m_last_tracknumber

2. 服务器A在掉线状态下将自身判定为工作机, 对收到R1、R2、R3的雷达报文, 以及模拟航迹报文进行处理

3. 当服务器A重连入主干网络后, 对于掉线过程中产生的航迹 [m_last_tracknumber, current_systrk_number), 将对应的融合单元作为报文转发给工作机, 让工作机处理
关联关系 

4. 服务器A的航迹回退到掉线前的状态

5. 对于接收到的离线数据, 工作机处理好服务器A的离线数据后会一起将所有的融合单元信息（通过收到的在线雷达数据和掉线服务器的离线数据生成）在下一周期发送给所有服务器

6. 所有服务器根据接收到的工作机的融合单元包更新数据

##### 问题：

服务器A在重连时发送的是最新的融合单元报文, 重连后工作机接收到的只是一个点, 因此重新关联后发送在显示程序上的也只是一个航迹点, 而不是一整条的完整航迹

目前能够保证服务器A掉线重连后航迹号与当前整个集群航迹号保持一致, 但是并不能输出完整的离线时接收到的模拟航迹

虽然当服务器重连后工作机能够通过发送的数据包建立离线数据和集群的航迹之间的对应关系, 但是好像并没有什么实质上的效果？
因为一旦当服务器A重连后, 它就会自动转为候补机, 候补机并不会进行融合航迹的处理
那么即使在重连后模拟数据依然发送给服务器A, 对于当前的整个集群来说, 这条模拟航迹依然是感知不到的（实际上目前重连后也并没有后续的模拟航迹发送
在超过设定的最大外推周期后（目前设定为4）, 系统航迹就会被删除, 建立的对应关系也随之删除
因此最终的实现效果和直接丢弃服务器A在离线过程中产生的航迹并无显著差别

##### 改进：

公网和各个服务器掉线后的独立网络用两套航迹系统, 公网上所有的服务器共同维护一套航迹系统, 独立网络各自维护一套航迹系统, 建立对应关系
服务器A掉线后作为工作机, 进行模拟数据的转发工作; 重连后作为候补机, 转发模拟数据

日志记录

服务器A发送的离线数据, 40为模拟航迹数据
```
[DEBUG]	[24-05-14 15:35:04.965]	[3166:3166]	[/root/MSDP_V0.3/msdp/Server.cpp:439]	[Send] offline track data: trkno[36] id[4] (215361.6873, -36565.1883)
[DEBUG]	[24-05-14 15:35:04.965]	[3166:3166]	[/root/MSDP_V0.3/msdp/Server.cpp:439]	[Send] offline track data: trkno[40] id[4] (-3013.0636, -323529.8109)
[DEBUG]	[24-05-14 15:35:04.965]	[3166:3166]	[/root/MSDP_V0.3/msdp/Server.cpp:439]	[Send] offline track data: trkno[41] id[4] (-39975.6099, -136865.8113)
[DEBUG]	[24-05-14 15:35:04.965]	[3166:3166]	[/root/MSDP_V0.3/msdp/Server.cpp:439]	[Send] offline track data: trkno[42] id[4] (-78017.2576, -152718.8417)
[DEBUG]	[24-05-14 15:35:04.965]	[3166:3166]	[/root/MSDP_V0.3/msdp/Server.cpp:439]	[Send] offline track data: trkno[43] id[4] (-45017.9360, -81896.2462)
[DEBUG]	[24-05-14 15:35:04.965]	[3166:3166]	[/root/MSDP_V0.3/msdp/Server.cpp:439]	[Send] offline track data: trkno[44] id[4] (-11920.9101, -34284.7345)
[DEBUG]	[24-05-14 15:35:04.965]	[3166:3166]	[/root/MSDP_V0.3/msdp/Server.cpp:439]	[Send] offline track data: trkno[45] id[4] (-11280.2269, -31815.4557)
[DEBUG]	[24-05-14 15:35:04.965]	[3166:3166]	[/root/MSDP_V0.3/msdp/Server.cpp:439]	[Send] offline track data: trkno[46] id[4] (-23989.1875, 61347.5000)
```

工作机的处理

接收到全部的离线数据
```
[DEBUG]	[24-05-14 15:35:05.570]	[2604:2612]	[/root/MSDP_V0.3/msdp/Server.cpp:355]	receive offline data trkno[36] id[4] (215361.6873, -36565.1883)
[DEBUG]	[24-05-14 15:35:05.570]	[2604:2612]	[/root/MSDP_V0.3/msdp/Server.cpp:355]	receive offline data trkno[40] id[4] (-3013.0636, -323529.8109)
[DEBUG]	[24-05-14 15:35:05.570]	[2604:2612]	[/root/MSDP_V0.3/msdp/Server.cpp:355]	receive offline data trkno[41] id[4] (-39975.6099, -136865.8113)
[DEBUG]	[24-05-14 15:35:05.570]	[2604:2612]	[/root/MSDP_V0.3/msdp/Server.cpp:355]	receive offline data trkno[42] id[4] (-78017.2576, -152718.8417)
[DEBUG]	[24-05-14 15:35:05.570]	[2604:2612]	[/root/MSDP_V0.3/msdp/Server.cpp:355]	receive offline data trkno[43] id[4] (-45017.9360, -81896.2462)
[DEBUG]	[24-05-14 15:35:05.570]	[2604:2612]	[/root/MSDP_V0.3/msdp/Server.cpp:355]	receive offline data trkno[44] id[4] (-11920.9101, -34284.7345)
[DEBUG]	[24-05-14 15:35:05.570]	[2604:2612]	[/root/MSDP_V0.3/msdp/Server.cpp:355]	receive offline data trkno[45] id[4] (-11280.2269, -31815.4557)
[DEBUG]	[24-05-14 15:35:05.570]	[2604:2612]	[/root/MSDP_V0.3/msdp/Server.cpp:355]	receive offline data trkno[46] id[4] (-23989.1875, 61347.5000)
```

为模拟航迹分配了新建的航迹号
```
[DEBUG]	[24-05-14 15:35:07.892]	[2604:2604]	[/root/MSDP_V0.3/msdp/fusion/MFsuion.cpp:263]	 [Fusion] new systrk create, systrk_no[42] id[4] unitrk_no[40] (-3583.1553, -3583.1553)
```

#### 新增 raft 后报错问题解决 20240710

1. 多重定义问题解决

用了一个轻量级的第三方 rpc 库 buttonrpc.hpp

声明和定义都写在了一个 .hpp文件中

照理来说应该没有什么问题但报了多重定义的错误，版本：CentOS7 gcc 9.3.1

由于实现的函数都比较轻量，因此在函数前添加了 inline 关键字, 得到解决（但是这种解决方法可能不太好，先这样再说

2. undefined 类解决

CMakeLists 中忘记添加了相关的 proto

修改前：
```
# 添加 .proto 文件
protobuf_generate_cpp(PROTO_SRCS PROTO_HDRS ${MSDP_PROTO_DIR}/fusion.proto)
```

修改后：
```
# 添加 .proto 文件
protobuf_generate_cpp(PROTO_SRCS PROTO_HDRS ${MSDP_PROTO_DIR}/fusion.proto ${MSDP_PROTO_DIR}/raft.proto)
```

#### 基于Raft的一致性算法实现
##### 领导选举


##### 日志复制
1. 对于日志不一致问题的处理：强制覆盖
```
1. 找到最后两者达成一致的地方(日志一致的判定：日志具有相同的索引和任期号)
2. 删除m_peers[peer_id]从那个点之后的所有日志条目
3. 将自己那个点之后的日志发送给 m_peers[id] ;
以上操作在进行附加日志RPC的一致性检查时完成
```

2. 一致性检查：
```
对于领导人：
1. 刚获得权力时，初始化所有的 nextIndex 为自己最后一条日志的 idx+1
2. 若一个跟随者的日志和领导人不一致，那么在下一次的附加日志 RPC 时的一致性检查就会失败。
在被跟随者拒绝之后，领导人就会减小 nextIndex 值并进行重试。
改进：
为实现快速匹配，在 reply 中的参数列表添加 last_term 和 last_idx
若 reply true, 这两个参数分别指向匹配到的日志条目的周期和日志索引
否则，指向出现冲突的日志条目和日志索引
```

3. LogAppendRequest 实现（完成）：
```
参数设置: 
term -- 领导人周期号
leader_id -- 领导人id
prev_log_idx -- 新的日志条目前紧挨着的条目的索引位置
prev_log_term -- 新的日志条目前紧挨着的条目的索任期号
logs -- 需要 append 的日志（作为心跳时为空, 发送时为提高效率，可一次发送多条日志）
commit_idx -- 已提交日志的索引

对于接收到的 reply: 
success: 
删除 nextIndex 后的所有条目（若有，该操作需要在receiver中实现）
将 nextIndex 后的所有日志发送给 follower （将 logs 传入到 rpc 的参数中，在下一次 rpc 调用中实现日志的添加）
更新 nextIndex 数组和 matchedIndex 数组

!success: 根据接收者的 reply 返回的 nextIndex 和 conflictTerm 调整 nextIndex 数组，在下一次 request 中进行重试
```

4. LogAppendReply 实现（已完成）:
```
1. 更新 reply 的 term 为 current_term
    args.term < current_term => reply.success = false return
2  若 args 的索引号更大，直接添加（？）
3. 出现匹配冲突, 返回 conflict_term 和 conflict_idx
4. 匹配成功, 对于 prevIndex 后的所有日志进行替换
5. 检验 若 commit_idx < leaderCommit, 设置本机的 commit_idx 为 min(leaderCommit, m_log.size())
```

4. commit 的条件：在领导人将创建的日志条目复制到大多数的服务器上的时候（半数通过）